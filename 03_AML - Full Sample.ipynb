{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b960e716",
   "metadata": {},
   "source": [
    "# Azure ML - Sample Batch Prediction Pipeline\n",
    "- Parallel run step leveraged\n",
    "- Output collected & saved to blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c598c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "import os, shutil\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData, PublishedPipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b5bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "folder_name = 'batch-inferencing-full'\n",
    "script_folder = os.path.join(os.getcwd(), folder_name)\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df03371",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d70365",
   "metadata": {},
   "source": [
    "## Create Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf6e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "\n",
    "compute_name =  \"email-cluster4\"\n",
    "print(compute_name)\n",
    "\n",
    "# checks to see if compute target already exists in workspace, else create it\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=compute_name)\n",
    "except ComputeTargetException:\n",
    "    config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D13_V2\",\n",
    "                                                   min_nodes=2, \n",
    "                                                   max_nodes=10)\n",
    "\n",
    "    compute_target = ComputeTarget.create(workspace=ws, name=compute_name, provisioning_configuration=config)\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92bf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/email_classification_inference.yml\n",
    "name: email_classification_inference\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "# Create an Environment for the experiment\n",
    "batch_env = Environment.from_conda_specification(\"email_classification_inference\", script_folder + \"/email_classification_inference.yml\")\n",
    "batch_env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "print('Configuration ready.')\n",
    "\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = batch_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d8bedb",
   "metadata": {},
   "source": [
    "# Define Output Datasets\n",
    "\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5627b",
   "metadata": {},
   "source": [
    "# Define Pipeline Parameters\n",
    "\n",
    "PipelineParameter objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify a pipeline parameter object model_name which will be used to reference the locally trained model that was uploaded and registered within the Azure ML workspace. Multiple pipeline parameters can be created and used. Included here are multiple sample pipeline parameters (get_data_param_*) to highlight how parameters can be passed into and consumed by various pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PipelineParameter(name='model_name', default_value='email_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a5ffe",
   "metadata": {},
   "source": [
    "# Define Pipeline Steps\n",
    "\n",
    "The pipeline below consists of steps to gather and register data from a remote source, a scoring step where the registered model is used to make predictions on loaded, and a data publish step where scored data can be exported to a remote data source. All of the PythonScriptSteps have a corresponding *.py file which is referenced in the step arguments. Also, any PipelineParameters defined above can be passed to and consumed within these steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "folder_name = 'batch-inferencing'\n",
    "script_folder = os.path.join(os.getcwd(), folder_name)\n",
    "print(script_folder)\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/batch_inferencing_data_silly.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Model\n",
    "import joblib\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def init():\n",
    "    # Runs when the pipeline step is initialized\n",
    "    global model\n",
    "\n",
    "    # load the model\n",
    "    print('****loaded model**********')\n",
    "    model_path = Model.get_model_path('email_classifier')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "def run(mini_batch):\n",
    "    # This runs for each batch\n",
    "    print(f'run method start: {__file__}, run({mini_batch})')\n",
    "    resultList = []\n",
    "    all_predictions = pd.DataFrame()\n",
    "    \n",
    "    for idx, file_path in enumerate(mini_batch):\n",
    "        file_name, file_extension = os.path.splitext(os.path.basename(file_path))\n",
    "       \n",
    "        #print(file_path)\n",
    "        #data = pd.read_csv(file_path)\n",
    "        \n",
    "        text_file = open(file_path, \"r\")\n",
    "        data = text_file.read()\n",
    "        text_file.close()\n",
    "        result = model.predict([data])\n",
    "        print(data)\n",
    "        resultList.append(\"{}: {}\".format(os.path.basename(file_path), result[0]))\n",
    "    #return resultList\n",
    "        \n",
    "        #for _, row in result_df.iterrows():\n",
    "        #    result_list.append((row))\n",
    "\n",
    "\n",
    "    #Return all rows formatted as a Pandas dataframe\n",
    "    return pd.DataFrame(resultList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b3a0d",
   "metadata": {},
   "source": [
    "You're going to use a pipeline to run the batch prediction script, generate predictions from the input data, and save the results as a text file in the output folder. To do this, you can use a **ParallelRunStep**, which enables the batch data to be processed in parallel and the results collated in a single output file named *parallel_run_step.txt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a dataset for the input data\n",
    "batch_data_set = Dataset.File.from_files(path=(default_ds, 'spam-data-inferencing/'), validate=False)\n",
    "try:\n",
    "    batch_data_set = batch_data_set.register(workspace=ws, \n",
    "                                             name='spam-batch-data-inference',\n",
    "                                             description='inference batch data',\n",
    "                                             create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708d7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/organize_data_silly.py\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "# Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"parallel run step results directory\")\n",
    "parser.add_argument(\"--processed_dataset_tabular\", dest='processed_dataset_tabular', required=True)\n",
    "parser.add_argument(\"--processed_dataset\", type=str, required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "#Get output data from previous step - saved as parallel_run_step.txt\n",
    "pipeline_data_file = os.path.join(args.processed_dataset, 'parallel_run_step.txt')\n",
    "\n",
    "#Parse as dataframe and assign headers\n",
    "df_pipeline_data = pd.read_csv(pipeline_data_file, header=None, delimiter=\" \")\n",
    "\n",
    "print(df_pipeline_data.columns)\n",
    "#df_pipeline_data.columns = ['D', 'E', 'F', 'G', 'A', 'B', 'C', 'Year']\n",
    "\n",
    "#Note: additional DF formatting operations can be done here\n",
    "\n",
    "#Create output directories for CSV/Excel files\n",
    "os.makedirs(args.processed_dataset_tabular, exist_ok=True)\n",
    "os.makedirs(args.processed_dataset, exist_ok=True)\n",
    "\n",
    "#Save output files to blob storage\n",
    "df_pipeline_data.to_csv(os.path.join(args.processed_dataset_tabular, 'processed_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "#output_dir = OutputFileDatasetConfig(name='inferences')\n",
    "processed_dataset_tabular = OutputFileDatasetConfig(name='processed_data_tabular', destination=(default_ds, 'processed_data_tabular/{run-id}')).read_delimited_files().register_on_complete(name='processed_data_tabular')\n",
    "#processed_dataset_file = OutputFileDatasetConfig(name='processed_data_file', destination=(default_ds, 'processed_data_file/{run-id}')).register_on_complete(name='processed_data_file')\n",
    "processed_dataset_pipeline_data = PipelineData(name='processed_data', datastore=default_ds)\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=script_folder,\n",
    "    entry_script=\"batch_inferencing_data_silly.py\",\n",
    "    mini_batch_size=\"50\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=batch_env,\n",
    "    compute_target=compute_target,\n",
    "    node_count=2)\n",
    "\n",
    "parallelrun_step = ParallelRunStep(\n",
    "    name='batch-score-diabetes',\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[batch_data_set.as_named_input('email_batch')],\n",
    "    output=processed_dataset_pipeline_data,\n",
    "    arguments=[],\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "organize_results_step = PythonScriptStep(\n",
    "    name='organize_results_step',\n",
    "    script_name='organize_data_silly.py',\n",
    "    arguments =['--processed_dataset_tabular', processed_dataset_tabular,\n",
    "               '--processed_dataset', processed_dataset_pipeline_data],\n",
    "    inputs=[processed_dataset_pipeline_data],\n",
    "    outputs=[processed_dataset_tabular],\n",
    "    compute_target=compute_target,\n",
    "    source_directory=script_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "print('Steps defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce05cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[parallelrun_step, organize_results_step])\n",
    "pipeline_run = Experiment(ws, '03-email-classifcation-batch-inference_full').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da212398",
   "metadata": {},
   "source": [
    "## Publish the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17da7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline = pipeline.publish(name = 'Email Batch Prediction Pipeline Silly',\n",
    "#                                      description = 'Pipeline that generates batch predictions using a registered trained model.',\n",
    "#                                      continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\n",
    "# pipeline_id = '2c8fc5ae-1508-4bf9-9dda-24c21fb2e8aa'\n",
    "# experiment_name = 'scheduled_silly_email'\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=5)\n",
    "# recurring_schedule = Schedule.create(ws, name=\"MyRecurringSchedule\", \n",
    "#                             description=\"Based on time\",\n",
    "#                             pipeline_id=pipeline_id, \n",
    "#                             experiment_name=experiment_name, \n",
    "#                             recurrence=recurrence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4073cf4",
   "metadata": {},
   "source": [
    "## Get published pipeline Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b584f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments = Experiment.list(ws)\n",
    "# # for experiment in experiments:\n",
    "# #     print(experiment.name)\n",
    "\n",
    "# published_pipelines = PublishedPipeline.list(ws)\n",
    "# for published_pipeline in  published_pipelines:\n",
    "#     print(f\"{published_pipeline.name},'{published_pipeline.id}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1f1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss = Schedule.list(ws)\n",
    "# for s in ss:\n",
    "#     print(s)\n",
    "#     print('****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stop_by_schedule_id(ws, schedule_id):\n",
    "#     s = next(s for s in Schedule.list(ws) if s.id == schedule_id)\n",
    "#     s.disable()\n",
    "#     return s\n",
    "\n",
    "# #stop_by_schedule_id(ws, '60166fcd-5276-4557-9a5b-c5a0ce3ec84e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabf2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = PublishedPipeline.get(ws, id = '898c1939-7278-4ce8-976f-106b71bbb678')\n",
    "# pipeline.disable()\n",
    "\n",
    "# # for published_pipeline in  published_pipelines:\n",
    "# #     pipeline = PublishedPipeline.get(ws, id = published_pipeline.id)\n",
    "# #     pipeline.disable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fdf4b",
   "metadata": {},
   "source": [
    "## Set Schedule for Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ab516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_id = published_pipeline.Id\n",
    "# experiment_name = 'silly_scheduled_email'\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Minute\", interval=5)\n",
    "# recurring_schedule = Schedule.create(ws, name=\"MyRecurringSchedule\", \n",
    "#                             description=\"Based on time\",\n",
    "#                             pipeline_id=pipeline_id, \n",
    "#                             experiment_name=experiment_name, \n",
    "#                             recurrence=recurrence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
